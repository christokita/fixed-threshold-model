expand = c(0, 0)) +
scale_size_continuous(range = c(1.5, 1.5)) +
theme(legend.position = "none") +
# Mean and SE portion of plot
geom_errorbar(data = stimSumFluct,
aes(x = n,
ymin = s1FluctMean - s1FluctSE,
ymax = s1FluctMean + s1FluctSE),
colour = "black",
width = 2) +
geom_point(data = stimSumFluct,
aes(x = n, y = s1FluctMean, size = size),
colour = "black") +
theme(legend.position = "none",
legend.justification = c(1, 1),
legend.title = element_blank(),
legend.key.height = unit(0.3, "cm"),
legend.key.width= unit(0.4, "cm"),
legend.margin =  margin(t = 0, r = 0, b = 0, l = -0.2, "cm"),
legend.text = element_text(size = 6),
legend.text.align = 0,
# legend.box.background = element_rect(),
axis.text = element_text(size = 8),
axis.title.y = element_text(size = 10, margin = margin(0, 11, 0, 0)),
axis.title.x = element_text(size = 10),
strip.text = element_blank(),
strip.background = element_blank(),
panel.spacing = unit(0.25, "cm")) +
facet_grid(. ~ mask, scales = "free", space = "free")
####################
# Plot
####################
# MultiPlot
png(filename = paste0("output/_ComprehnsivePlots/", filename, ".png"), width = 6, height = 6, units = "in", res = 800)
multiplot(gg_dist, gg_mean, gg_stimfluct,
gg_corr, gg_varNorm, gg_fluct,
cols = 2)
dev.off()
gg_stimfluct
stimSumFluct
# Summarise by n
stimSumFluct <- stimFluct %>%
group_by(n, GroupSizeFactor) %>%
summarise(s1FluctMean = mean(s1Fluct, na.rm = TRUE),
s1FluctSE = sd(s1Fluct, na.rm = TRUE) / sqrt(length(s1Fluct)),
s2FluctMean = mean(s2Fluct, na.rm = TRUE),
s2FluctSE = sd(s2Fluct, na.rm = TRUE) / sqrt(length(s2Fluct)),
sFluctMean = mean(sFluct, na.rm = TRUE),
sFluctSE = sd(sFluct, na.rm = TRUE) / sqrt(length(sFluct)))
stimSumFluct <- as.data.frame(stimSumFluct)
stimSumFluct <- stimSumFluct %>%
mutate(GroupSizeFactor = factor(GroupSizeFactor, levels = sort(unique(n))))
addrows <- data.frame(n = c(37, 95, 102),
GroupSizeFactor = rep(NA, 2),
s1FluctMean = c(0.1220691, 0.0701031, 0.0701031),
s1FluctSE = c(NA, NA, NA),
s2FluctMean = c(NA, NA, NA),
s2FluctSE= c(NA, NA, NA),
sFluctMean = c(NA, NA, NA),
sFluctSE = c(NA, NA, NA))
addrows <- data.frame(n = c(37, 95, 102),
GroupSizeFactor = rep(NA, 3),
s1FluctMean = c(0.1220691, 0.0701031, 0.0701031),
s1FluctSE = c(NA, NA, NA),
s2FluctMean = c(NA, NA, NA),
s2FluctSE= c(NA, NA, NA),
sFluctMean = c(NA, NA, NA),
sFluctSE = c(NA, NA, NA))
stimSumFluct <- rbind(stimSumFluct, addrows)
stimSumFluct$mask <- 0
stimSumFluct$mask[stimSumFluct$n > 90] <- 1
stimFluct$mask <- 0
stimFluct$mask[stimFluct$n > 90] <- 1
stimSumFluct$size <- 1.5
stimSumFluct$size[stimSumFluct$n < 100 & stimSumFluct$n > 35] <- NA
stimSumFluct$size[stimSumFluct$n > 100] <- NA
# Plot
gg_stimfluct <- ggplot() +
geom_point(data = stimFluct,
aes(x = n, y = s1Fluct),
fill = "grey50",
colour = "grey50",
position = position_jitter(width = 0.1),
size = 0.7,
alpha = 0.4,
stroke = 0) +
geom_line(data = stimSumFluct,
aes(x = n, y = s1FluctMean),
size = 0) +
theme_classic() +
labs(x = "Group size",
y = "Stimulus fluctuation") +
scale_x_continuous(breaks = unique(stimFluct$n)) +
scale_y_continuous(breaks = seq(0, 2, 0.2),
limits = c(0, 0.85),
expand = c(0, 0)) +
scale_size_continuous(range = c(1.5, 1.5)) +
theme(legend.position = "none") +
# Mean and SE portion of plot
geom_errorbar(data = stimSumFluct,
aes(x = n,
ymin = s1FluctMean - s1FluctSE,
ymax = s1FluctMean + s1FluctSE),
colour = "black",
width = 2) +
geom_point(data = stimSumFluct,
aes(x = n, y = s1FluctMean, size = size),
colour = "black") +
theme(legend.position = "none",
legend.justification = c(1, 1),
legend.title = element_blank(),
legend.key.height = unit(0.3, "cm"),
legend.key.width= unit(0.4, "cm"),
legend.margin =  margin(t = 0, r = 0, b = 0, l = -0.2, "cm"),
legend.text = element_text(size = 6),
legend.text.align = 0,
# legend.box.background = element_rect(),
axis.text = element_text(size = 8),
axis.title.y = element_text(size = 10, margin = margin(0, 11, 0, 0)),
axis.title.x = element_text(size = 10),
strip.text = element_blank(),
strip.background = element_blank(),
panel.spacing = unit(0.25, "cm")) +
facet_grid(. ~ mask, scales = "free", space = "free")
####################
# Plot
####################
# MultiPlot
png(filename = paste0("output/_ComprehnsivePlots/", filename, ".png"), width = 6, height = 6, units = "in", res = 800)
multiplot(gg_dist, gg_mean, gg_stimfluct,
gg_corr, gg_varNorm, gg_fluct,
cols = 2)
dev.off()
# Summarise by n
tallySumFluct <- tallyFluct %>%
group_by(n, GroupSizeFactor) %>%
summarise(Task1FluctMean = mean(Task1Fluct, na.rm = TRUE),
Task1FluctSE = sd(Task1Fluct) / sqrt(length(Task1Fluct)),
Task2FluctMean = mean(Task2Fluct, na.rm = TRUE),
Task2FluctSE = sd(Task2Fluct, na.rm = TRUE) / sqrt(length(Task2Fluct)),
InactiveFluctMean = mean(InactiveFluct, na.rm = TRUE),
InactiveFluctSE = sd(InactiveFluct, na.rm = TRUE) / sqrt(length(InactiveFluct)))
tallySumFluct <- as.data.frame(tallySumFluct)
tallySumFluct <- tallySumFluct %>%
mutate(GroupSizeFactor = factor(GroupSizeFactor, levels = sort(unique(n))))
addrows <- data.frame(n = c(37, 95),
GroupSizeFactor = rep(NA, 3),
Task1FluctMean = c(0.04920449, 0.03104808, 0.03104808),
Task1FluctSE = c(NA, NA, NA),
Task2FluctMean = c(NA, NA, NA),
Task2FluctSE= c(NA, NA, NA),
InactiveFluctMean = c(NA, NA, NA),
InactiveFluctSE = c(NA, NA, NA))
tallySumFluct <- rbind(tallySumFluct, addrows)
tallySumFluct$mask <- 0
tallySumFluct$mask[stimSumFluct$n > 90] <- 1
tallyFluct$mask <- 0
tallyFluct$mask[tallyFluct$n > 90] <- 1
tallySumFluct$size <- 1.5
tallySumFluct$size[stimSumFluct$n < 100 & stimSumFluct$n > 35] <- NA
stimSumFluct$size[stimSumFluct$n > 100] <- NA
# Summarise by n
tallySumFluct <- tallyFluct %>%
group_by(n, GroupSizeFactor) %>%
summarise(Task1FluctMean = mean(Task1Fluct, na.rm = TRUE),
Task1FluctSE = sd(Task1Fluct) / sqrt(length(Task1Fluct)),
Task2FluctMean = mean(Task2Fluct, na.rm = TRUE),
Task2FluctSE = sd(Task2Fluct, na.rm = TRUE) / sqrt(length(Task2Fluct)),
InactiveFluctMean = mean(InactiveFluct, na.rm = TRUE),
InactiveFluctSE = sd(InactiveFluct, na.rm = TRUE) / sqrt(length(InactiveFluct)))
tallySumFluct <- as.data.frame(tallySumFluct)
tallySumFluct <- tallySumFluct %>%
mutate(GroupSizeFactor = factor(GroupSizeFactor, levels = sort(unique(n))))
addrows <- data.frame(n = c(37, 95, 102),
GroupSizeFactor = rep(NA, 3),
Task1FluctMean = c(0.04920449, 0.03104808, 0.03104808),
Task1FluctSE = c(NA, NA, NA),
Task2FluctMean = c(NA, NA, NA),
Task2FluctSE= c(NA, NA, NA),
InactiveFluctMean = c(NA, NA, NA),
InactiveFluctSE = c(NA, NA, NA))
tallySumFluct <- rbind(tallySumFluct, addrows)
tallySumFluct$mask <- 0
tallySumFluct$mask[stimSumFluct$n > 90] <- 1
tallyFluct$mask <- 0
tallyFluct$mask[tallyFluct$n > 90] <- 1
tallySumFluct$size <- 1.5
tallySumFluct$size[stimSumFluct$n < 100 & stimSumFluct$n > 35] <- NA
stimSumFluct$size[stimSumFluct$n > 100] <- NA
tallySumFluct$mask <- 0
tallySumFluct$mask[stimSumFluct$n > 90] <- 1
tallyFluct$mask <- 0
tallyFluct$mask[tallyFluct$n > 90] <- 1
tallySumFluct$size <- 1.5
tallySumFluct$size[tallySumFluct$n < 100 & tallySumFluct$n > 35] <- NA
tallySumFluct$size[tallySumFluct$n > 100] <- NA
# Plot
gg_fluct <- ggplot() +
geom_point(data = tallyFluct,
aes(x = n, y = Task1Fluct),
fill = "grey50",
colour = "grey50",
size = 0.7,
position = position_jitter(width = 0.1),
alpha = 0.4,
stroke = 0) +
theme_classic() +
labs(x = "Group size",
y = "Task fluctuation") +
scale_x_continuous(breaks = unique(tallyFluct$n)) +
scale_y_continuous(breaks = seq(0, 0.22, 0.02),
limits = c(0, 0.155),
expand = c(0, 0)) +
scale_size_continuous(range = c(1.5, 1.5)) +
theme(legend.position = "none") +
# Mean and SE portion of plot
geom_errorbar(data = tallySumFluct,
aes(x = n,
ymin = Task1FluctMean - Task1FluctSE,
ymax = Task1FluctMean + Task1FluctSE),
colour= "black",
width = 2) +
geom_point(data = tallySumFluct,
aes(x = n, y = Task1FluctMean, size = size),
colour = "black") +
theme(legend.position = "none",
legend.justification = c(1, 1),
legend.title = element_blank(),
legend.key.height = unit(0.3, "cm"),
legend.key.width= unit(0.4, "cm"),
legend.margin =  margin(t = 0, r = 0, b = 0, l = -0.2, "cm"),
legend.text = element_text(size = 6),
legend.text.align = 0,
# legend.box.background = element_rect(),
axis.text = element_text(size = 8),
axis.title = element_text(size = 10, margin = margin(0, 0, 0, 0)),
strip.text = element_blank(),
strip.background = element_blank(),
panel.spacing = unit(0.25, "cm"))  +
facet_grid(. ~ mask, scales = "free", space = "free")
####################
# Plot
####################
# MultiPlot
png(filename = paste0("output/_ComprehnsivePlots/", filename, ".png"), width = 6, height = 6, units = "in", res = 800)
multiplot(gg_dist, gg_mean, gg_stimfluct,
gg_corr, gg_varNorm, gg_fluct,
cols = 2)
dev.off()
rm(list = ls())
source("scripts/__Util__MASTER.R")
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- c(16) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 10000 #number of generations to run simulation
corrStep       <- 200 #number of time steps for calculation of correlation
reps           <- 1 #number of replications per simulation (for ensemble) !!Change!!
# Threshold Parameters
ThreshM        <- c(10, 10) #population threshold means
ThreshSD       <- ThreshM * 0.15 #population threshold standard deviations !!Change!!
InitialStim    <- c(0, 0) #intital vector of stimuli
StimRates      <- c(0.6, 0.6) #vector of stimuli increase rates
threshSlope    <- 30 #exponent parameter for threshold curve shape
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
filename <- "MSrevision_FixedDelta06Sigma01Eta7_Sigma0Eta30_Test"
####################
# Run simulation multiple times
####################
# Prep meta-lists for collection of group size simulations
groups_taskDist  <- list()
groups_taskCorr  <- list()
groups_taskStep  <- list()
groups_taskTally <- list()
groups_stim      <- list()
groups_entropy   <- list()
groups_specialization <- data.frame(NULL)
groups_taskOverTime <- list()
# Loop through group sizes
for (i in 1:length(Ns)) {
# Set group size
n <- Ns[i]
# Prep lists for collection of simulation outputs
ens_taskDist  <- list()
ens_taskCorr  <- list()
ens_taskStep  <- list()
ens_taskTally <- list()
ens_entropy   <- list()
ens_stim      <- list()
ens_taskOverTime <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- initiateProbMatrix(n = n, m = m)
# Seed task (external) stimuli
stimMat <- seedStimuls(InitialSVector = InitialStim,
RateVector = StimRates,
gens = gens)
# Seed internal thresholds
threshMat <- seedThresholds(n = n,
m = m,
ThresholdMeans = ThreshM,
ThresholdSDs = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Prep correlation step matrix
X_prev <- matrix(data = rep(0, n * m), ncol = m)
X_prevTot <- matrix(data = rep(0, n * m), ncol = m)
taskCorr <- list()
taskStep <- list()
taskTally <- list()
taskOverTime  <- matrix(nrow = 0, ncol = n)
####################
# Simulate
####################
# Run simulation
for (t in 1:gens) {
# Update stimuli
for (j in 1:(ncol(stimMat)/2)) {
# update stim
stimMat[t + 1, j] <- globalStimUpdate(stimulus = stimMat[t, j],
delta = stimMat[t, j + m],
alpha = alpha,
Ni = sum(X_g[ , j]),
n = n)
# shift down delta (rate increases)
stimMat[t + 1, j + m] <- stimMat[t, j + m]
}
# Calculate task demand based on global stimuli
P_g <- calcThresholdProbMat(TimeStep = t + 1, # first row is generation 0
ThresholdMatrix = threshMat,
StimulusMatrix = stimMat,
nSlope = threshSlope)
# Update task performance
X_g <- updateTaskPerformance(P_sub_g    = P_g,
TaskMat    = X_g,
QuitProb   = quitP)
# Note which task is being peformed
taskPerf <- matrix(nrow = 1, ncol = n)
for (l in 1:nrow(X_g)) {
task <- unname(which(X_g[l, ] == 1))
if (length(task) == 0) {
task <- 0
}
taskPerf[l] <- task
}
colnames(taskPerf) <- row.names(X_g)
taskOverTime <- rbind(taskOverTime, taskPerf)
# Capture current task performance tally
tally <- matrix(c(t, colSums(X_g)), ncol = ncol(X_g) + 1)
colnames(tally) <- c("t", colnames(X_g))
tally <- transform(tally, Inactive = n - sum(X_g), n = n, replicate = sim)
taskTally[[t]] <- tally
# Update total task performance profile
X_tot <- X_tot + X_g
# Create time step for correlation
if (t %% corrStep == 0) {
# Get tasks performance in correlation step
X_step <- X_tot - X_prevTot
# Add to ensemble list of task steps
taskStep[[t / corrStep]] <- X_step
# Calculate rank correlation if it is not the first step
if(sum(X_prev) != 0) {
# Normalize
stepNorm <- X_step / rowSums(X_step)
prevNorm <- X_prev / rowSums(X_prev)
# Calculate ranks
step_ranks <- calculateTaskRank(TaskStepMat = X_step)
prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)
# Calculate Correlation
rankCorr <- cor(prev_ranks, step_ranks, method = "spearman")
# Put in list
taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)
names(taskCorr)[(t / corrStep) - 1] <- paste0("Gen", t)
}
# Update previous step total matrix
X_prevTot <- X_tot
# Update previous step total matrix
X_prev <- X_step
}
}
# Calculate specialization of task performance
# from Gautrais et al. (2002)
for (col in 1:ncol(taskOverTime)) {
# Grab column of individual
t_prof <- taskOverTime[ , col ]
# Remove inactivity
t_prof <- paste(t_prof, collapse = "")
# Calculate transitions
t_prof <- gsub("1+", "1", t_prof)
t_prof <- gsub("2+", "2", t_prof)
t_prof <- gsub("0+", "", t_prof)
t_prof <- as.numeric(unlist(strsplit(as.character(t_prof), "")))
transitions <- lapply(2:length(t_prof), function(entry) {
a <- t_prof[entry] != t_prof[entry - 1]
})
C_i <- sum(unlist(transitions))
C_i <- C_i / (length(t_prof) - 1)
# Calulate specialization
F_i <- 1 - m * C_i
to_return <- data.frame(individual = paste0("v-", col),
n = n,
replicate = sim,
TransSpec = F_i)
groups_specialization <- rbind(groups_specialization, to_return)
}
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- transform(entropy, n = n, replicate = sim)
# Calculate total task distribution
# totalTaskDist <- X_tot / rowSums(X_tot)
totalTaskDist <- X_tot / gens
totalTaskDist <- transform(totalTaskDist, Inactive = gens - rowSums(X_tot), n = n, replicate = sim)
totalTaskDist$individual <- paste0("v-", 1:nrow(totalTaskDist))
# Create tasktally table
taskTally <- do.call("rbind", taskTally)
# Create tasktally table
stimMat <- transform(stimMat, n = n, replicate = sim)
# Create tasktally table
taskCorr <- transform(taskCorr, replicate = sim)
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]  <- totalTaskDist
ens_entropy[[sim]]   <- entropy
ens_taskCorr[[sim]]  <- taskCorr
ens_taskTally[[sim]] <- taskTally
ens_taskStep[[sim]]  <- taskStep
ens_stim[[sim]]      <- stimMat
ens_taskOverTime[[sim]] <- as.data.frame(taskOverTime)
# Print simulation completed
print(paste0("DONE: N = ", n, ", Simulation ", sim))
}
# Calculate mean correlation for each n
runCorrs <- lapply(ens_taskCorr, function(x) {
# Unlist
runs <- do.call("rbind", x)
replicate <- runs[nrow(runs), ]
replicate <- unique(replicate)
runs <- runs[-nrow(runs), ]
# Calculate mean
runMean <- matrix(data = rep(NA, m), ncol =  m)
for (column in 1:m) {
runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)
}
runMean <- cbind(runMean, replicate)
colnames(runMean) <- c("Task1", "Task2", "replicate")
return(runMean)
})
runCorrs <- do.call("rbind", runCorrs)
runCorrs <- transform(runCorrs, n = n)
# Add to list of lists
groups_taskDist[[i]]  <- ens_taskDist
groups_taskCorr[[i]]  <- runCorrs
groups_taskStep[[i]]  <- ens_taskStep
groups_taskTally[[i]] <- ens_taskTally
groups_stim[[i]]      <- ens_stim
groups_entropy[[i]]   <- ens_entropy
groups_taskOverTime[[i]] <- ens_taskOverTime
}
# trim out correlations for group size 1
if(1 %in% Ns) {
groups_taskCorr <- groups_taskCorr[-1]
}
col = 1
# Grab column of individual
t_prof <- taskOverTime[ , col ]
taskOverTime
# Remove inactivity
t_prof <- paste(t_prof, collapse = "")
t_prof
# Calculate transitions
t_prof <- gsub("1+", "1", t_prof)
t_prof <- gsub("2+", "2", t_prof)
t_prof <- gsub("0+", "", t_prof)
t_prof
# Grab column of individual
t_prof <- taskOverTime[ , col ]
# Remove inactivity
t_prof <- paste(t_prof, collapse = "")
# Calculate transitions
t_prof <- gsub("1+", "1", t_prof)
t_prof <- gsub("2+", "2", t_prof)
t_prof <- gsub("0+", "0", t_prof)
t_prof
t_prof <- gsub("0+", "", t_prof)
t_prof
t_prof <- as.numeric(unlist(strsplit(as.character(t_prof), "")))
t_prof
entry = 2
t_prof[entry] != t_prof[entry - 1]
entry  = 6
t_prof[entry] != t_prof[entry - 1]
transitions <- lapply(2:length(t_prof), function(entry) {
a <- t_prof[entry] != t_prof[entry - 1]
})
transition
transitions
C_i <- sum(unlist(transitions))
C_i
C_i <- C_i / (length(t_prof) - 1)
C_i
# Calulate specialization
F_i <- 1 - m * C_i
F_i
